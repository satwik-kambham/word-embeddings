{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning as L\n",
    "import torchmetrics as tm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\"\n",
    "DATA_DIR = \".data\"\n",
    "SPLITS = [\"train\", \"valid\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file from DATA_URL and extract it\n",
    "def download_and_extract(url, data_dir, force=False):\n",
    "    # Create the data directory if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # Get filename from URL\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Download the zip file and save it to disk\n",
    "    if not os.path.exists(filepath) or force:\n",
    "        print(f\"Downloading {url} to {filepath}\")\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    else:\n",
    "        print(f\"File {filepath} already downloaded\")\n",
    "\n",
    "    # Extract the zip file\n",
    "    if not os.path.exists(os.path.join(data_dir, \"wikitext-2\")) or force:\n",
    "        print(f\"Extracting {filepath} to {data_dir}\")\n",
    "        with zipfile.ZipFile(filepath, \"r\") as f:\n",
    "            f.extractall(data_dir)\n",
    "    else:\n",
    "        print(f\"File {filepath} already extracted\")\n",
    "\n",
    "    return os.path.join(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File .data/wikitext-2-v1.zip already downloaded\n",
      "File .data/wikitext-2-v1.zip already extracted\n"
     ]
    }
   ],
   "source": [
    "data_dir = download_and_extract(DATA_URL, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir, split):\n",
    "    assert split in SPLITS, f\"split must be one of {SPLITS}\"\n",
    "    filepath = os.path.join(data_dir, f\"wikitext-2/wiki.{split}.tokens\")\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = split_words(text)\n",
    "text = [token for token in text if token != \"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=',\n",
       " 'Valkyria',\n",
       " 'Chronicles',\n",
       " 'III',\n",
       " '=',\n",
       " 'Senjō',\n",
       " 'no',\n",
       " 'Valkyria',\n",
       " '3',\n",
       " ':']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:len(text) // 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def build(self, text):\n",
    "        for word in text:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.word2idx[word] for word in text if word in self.word2idx]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return [self.idx2word[idx] for idx in tokens if idx in self.idx2word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "vocab.build(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert text[:10] == vocab.decode(vocab.encode(text[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(text):\n",
    "    counts = {}\n",
    "    for word in text:\n",
    "        if word not in counts:\n",
    "            counts[word] = 0\n",
    "        counts[word] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = get_word_counts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_word(count, total, threshold=1e-3):\n",
    "        prob = 1 - math.sqrt(threshold * total / count)\n",
    "        return random.random() > prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499321,\n",
       " [('the', 28049),\n",
       "  (',', 25449),\n",
       "  ('.', 18228),\n",
       "  ('of', 14406),\n",
       "  ('and', 12597),\n",
       "  ('in', 9946),\n",
       "  ('to', 9676),\n",
       "  ('a', 8406),\n",
       "  ('=', 7358),\n",
       "  ('\"', 7111)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [token for token in text if keep_word(word_counts[token], len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341238,\n",
       " [('the', 3862),\n",
       "  (',', 3602),\n",
       "  ('.', 3003),\n",
       "  ('of', 2621),\n",
       "  ('and', 2568),\n",
       "  ('in', 2167),\n",
       "  ('to', 2152),\n",
       "  ('a', 2070),\n",
       "  ('=', 1901),\n",
       "  ('\"', 1824)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), sorted(get_word_counts(text).items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre_contexts(text, window_size=5):\n",
    "    centre_contexts = []\n",
    "    for i in range(window_size, len(text) - window_size):\n",
    "        centre = text[i]\n",
    "        context = text[i-window_size:i] + text[i+1:i+window_size+1]\n",
    "        centre_contexts.append((centre, context))\n",
    "    return centre_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37, [34, 36, 38, 39]),\n",
       " (38, [36, 37, 39, 40]),\n",
       " (39, [37, 38, 40, 42]),\n",
       " (40, [38, 39, 42, 43]),\n",
       " (42, [39, 40, 43, 23])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_centre_contexts(vocab.encode(text[30:50]), 2)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, corpus, cache_size=100000):\n",
    "        self.word_counts = get_word_counts(corpus)\n",
    "        self.words = list(self.word_counts.keys())\n",
    "        self.sampling_weights = [self.word_counts[word] ** 0.75 for word in self.words]\n",
    "        self.cumulative_weights = list(itertools.accumulate(self.sampling_weights))\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = random.choices(self.words, cum_weights=self.cumulative_weights, k=self.cache_size)\n",
    "        self.cache_idx = 0\n",
    "\n",
    "    def sample_negative(self, contexts, k=5):\n",
    "        negatives = []\n",
    "        for _ in range(k):\n",
    "            while True:\n",
    "                word = self.cache[self.cache_idx]\n",
    "                self.cache_idx = self.cache_idx + 1\n",
    "\n",
    "                if self.cache_idx >= self.cache_size:\n",
    "                    self.cache = random.choices(self.words, cum_weights=self.cumulative_weights, k=self.cache_size)\n",
    "                    self.cache_idx = 0\n",
    "\n",
    "                if word not in contexts:\n",
    "                    negatives.append(word)\n",
    "                    break\n",
    "        return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10661, 3589, 13138, 6689, 11654]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = Sampler(vocab.encode(text), cache_size=10000)\n",
    "sampler.sample_negative([\"the\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre_contexts_negatives_pairs(centre_contexts, sampler, negative_per_context=5):\n",
    "    pairs = []\n",
    "    for centre, contexts in tqdm(centre_contexts):\n",
    "        for context in contexts:\n",
    "            pairs.append((centre, context, 1))\n",
    "            negatives = sampler.sample_negative(contexts, k=negative_per_context)\n",
    "            for negative in negatives:\n",
    "                pairs.append((centre, negative, 0))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre_contexts_negatives_pairs(centre_contexts, sampler, negative_per_context=5):\n",
    "    centres = []\n",
    "    contexts_and_negatives = []\n",
    "    labels = []\n",
    "    for centre, contexts in tqdm(centre_contexts):\n",
    "        for context in contexts:\n",
    "            centres.append(centre)\n",
    "            contexts_and_negatives.append(context)\n",
    "            labels.append(1)\n",
    "            negatives = sampler.sample_negative(contexts, k=negative_per_context)\n",
    "            for negative in range(negative_per_context):\n",
    "                centres.append(centre)\n",
    "                contexts_and_negatives.append(negatives[negative])\n",
    "                labels.append(0)\n",
    "    return centres, contexts_and_negatives, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 10\n",
    "NEGATIVE_PER_CONTEXT = 5\n",
    "\n",
    "# centre_contexts = get_centre_contexts(vocab.encode(text), window_size=WINDOW_SIZE)\n",
    "# pairs = get_centre_contexts_negatives_pairs(\n",
    "#     centre_contexts, sampler, negative_per_context=NEGATIVE_PER_CONTEXT\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre_contexts_negatives_pairs_lazy(centre_contexts, sampler, negative_per_context=5):\n",
    "    for centre, contexts in centre_contexts:\n",
    "        for context in contexts:\n",
    "            yield centre, context, 1\n",
    "            negatives = sampler.sample_negative(contexts, k=negative_per_context)\n",
    "            for negative in range(negative_per_context):\n",
    "                yield centre, negatives[negative], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 10\n",
    "NEGATIVE_PER_CONTEXT = 5\n",
    "centre_contexts = get_centre_contexts(vocab.encode(text), window_size=WINDOW_SIZE)\n",
    "pairs_len = len(centre_contexts) * WINDOW_SIZE * 2 * (1 + NEGATIVE_PER_CONTEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        centre_contexts,\n",
    "        sampler,\n",
    "        negative_per_context,\n",
    "        pairs_len,\n",
    "        vocab,\n",
    "    ):\n",
    "        self.centre_contexts = centre_contexts\n",
    "        self.sampler = sampler\n",
    "        self.negative_per_context = negative_per_context\n",
    "        self.pair_iter = get_centre_contexts_negatives_pairs_lazy(\n",
    "            centre_contexts, sampler, negative_per_context=negative_per_context\n",
    "        )\n",
    "        self.pairs_len = pairs_len\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return next(self.pair_iter)\n",
    "        except StopIteration:\n",
    "            self.pair_iter = get_centre_contexts_negatives_pairs_lazy(\n",
    "                self.centre_contexts, self.sampler, negative_per_context=self.negative_per_context\n",
    "            )\n",
    "            return next(self.pair_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Word2VecDataset(\n",
    "    centre_contexts,\n",
    "    sampler,\n",
    "    negative_per_context=NEGATIVE_PER_CONTEXT,\n",
    "    pairs_len=pairs_len,\n",
    "    vocab=vocab,\n",
    ")\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        def init_weights(module):\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "        self.accuracy = tm.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, word, context):\n",
    "        word = self.word_embedding(word)\n",
    "        context = self.context_embedding(context)\n",
    "        return (word * context).sum(dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        centre, context, label = batch\n",
    "        y_hat = self(centre, context)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(y_hat, label.float())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        pred = torch.round(torch.sigmoid(y_hat))\n",
    "        self.accuracy(pred, label)\n",
    "        self.log(\"train_acc\", self.accuracy, prog_bar=False, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | word_embedding    | Embedding      | 3.0 M \n",
      "1 | context_embedding | Embedding      | 3.0 M \n",
      "2 | accuracy          | BinaryAccuracy | 0     \n",
      "-----------------------------------------------------\n",
      "5.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 M     Total params\n",
      "23.642    Total estimated model params size (MB)\n",
      "/home/satwik/mambaforge/envs/insight/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54609a84c0da4901bbeaee27368b4dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satwik/mambaforge/envs/insight/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingModel(len(vocab.word2idx), 128)\n",
    "trainer = L.Trainer(max_epochs=5)\n",
    "trainer.fit(model, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (word_embedding): Embedding(23088, 128)\n",
       "  (context_embedding): Embedding(23088, 128)\n",
       "  (accuracy): BinaryAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    idx = vocab.word2idx[word]\n",
    "    return word_embedding(torch.tensor(idx)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embed_a, embed_b):\n",
    "    return np.dot(embed_a, embed_b) / (\n",
    "        np.linalg.norm(embed_a) * np.linalg.norm(embed_b)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70177114"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = (\"TV\", \"comedy\")\n",
    "embeddings = [get_embedding(word) for word in words]\n",
    "cosine_similarity(*embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9999999, 'star'),\n",
       " (0.7052185, 'playing'),\n",
       " (0.6851464, 'starred'),\n",
       " (0.65827155, 'cast'),\n",
       " (0.65029514, 'comedy'),\n",
       " (0.6498021, 'TV'),\n",
       " (0.64620864, 'appeared'),\n",
       " (0.6402657, 'Sonia'),\n",
       " (0.6315743, 'starring'),\n",
       " (0.6226327, 'television')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cosine similarity between all words and given word\n",
    "word = \"star\"\n",
    "embeddings = get_embedding(word)\n",
    "cosine_similarities = [\n",
    "    cosine_similarity(embeddings, get_embedding(word)) for word in vocab.word2idx\n",
    "]\n",
    "\n",
    "# Sort and get top 10 most similar words and their cosine similarity\n",
    "sorted_cosine_similarities = sorted(\n",
    "    zip(cosine_similarities, vocab.word2idx), reverse=True\n",
    ")\n",
    "sorted_cosine_similarities[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
